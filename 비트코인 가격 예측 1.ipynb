{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0063ad50-0096-47fd-84cf-69097bc50731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   timeOpen                 timeClose  \\\n",
      "0  2024-11-29T00:00:00.000Z  2024-11-29T23:59:59.999Z   \n",
      "1  2024-11-28T00:00:00.000Z  2024-11-28T23:59:59.999Z   \n",
      "2  2024-11-27T00:00:00.000Z  2024-11-27T23:59:59.999Z   \n",
      "3  2024-11-26T00:00:00.000Z  2024-11-26T23:59:59.999Z   \n",
      "4  2024-11-25T00:00:00.000Z  2024-11-25T23:59:59.999Z   \n",
      "\n",
      "                   timeHigh                   timeLow  name          open  \\\n",
      "0  2024-11-29T15:21:00.000Z  2024-11-29T00:54:00.000Z  2781  95653.952594   \n",
      "1  2024-11-28T01:01:00.000Z  2024-11-28T10:10:00.000Z  2781  95954.944882   \n",
      "2  2024-11-27T20:31:00.000Z  2024-11-27T00:47:00.000Z  2781  91978.138582   \n",
      "3  2024-11-26T06:28:00.000Z  2024-11-26T20:40:00.000Z  2781  93087.279992   \n",
      "4  2024-11-25T09:23:00.000Z  2024-11-25T22:40:00.000Z  2781  98033.442700   \n",
      "\n",
      "           high           low         close        volume     marketCap  \\\n",
      "0  98693.168167  95407.886372  97461.523713  5.496868e+10  1.928821e+12   \n",
      "1  96650.203768  94677.354296  95652.465159  5.226001e+10  1.892869e+12   \n",
      "2  97361.181936  91778.661212  95962.528419  7.113345e+10  1.898775e+12   \n",
      "3  94991.749969  90770.815041  91985.316830  9.165652e+10  1.820054e+12   \n",
      "4  98935.031807  92642.914861  93102.295214  8.090946e+10  1.842257e+12   \n",
      "\n",
      "                  timestamp  \n",
      "0  2024-11-29T23:59:59.999Z  \n",
      "1  2024-11-28T23:59:59.999Z  \n",
      "2  2024-11-27T23:59:59.999Z  \n",
      "3  2024-11-26T23:59:59.999Z  \n",
      "4  2024-11-25T23:59:59.999Z  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "                                        #데이터는 coinmarketcap 이라는 사이트에서 최근 1년치 비트 코인의 가격들(open, high, low, close, volume, marketcap)이 있는 데이터들을 csv로 다운받았습니다\n",
    "file_path = '/Users/chj/Downloads/Bitcoin_2023. 12. 2.-2024. 12. 1._historical_data_coinmarketcap.csv'  # CSV 파일 경로\n",
    "btc_data = pd.read_csv(file_path, sep = ';')  #열 이름을 출력한 결과를 보니, 모든 열 이름이 하나의 문자열로 묶여 있었다. 이는 CSV 파일이 잘못 구분되어 불러와졌을 가능성이 있으므로. ;(세미콜론)으로 구분된 파일이 쉼표(,)로 구분된 파일처럼 불러와져서 열 이름이 하나로 합쳐졌다. sep 인자를 사용해서 ; 로 구분하게한다. \n",
    "\n",
    "print(btc_data.head())          #각 열 마다 위의 5개의 데이터들을 출력. 데이터가 잘 불러와졌는지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a93a764-8538-4ecc-a0ed-99841c9a5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 특징,feature 선택한다. 다음날의 마지막 비트코인 가격을 예측하는 것이기 때문에 1년치 데이터들중에 가격들(open ,high ,low ,close ,volume(거래량), marketCap(시가총액 = 현재 비트코인 가격 * 유통중인 비트코인 수))만 추출한다. 시가총액은 비트코인의 시장 내 상대적인 규모를 나타내며, 다른 암호화폐와 비교할 때 안정성이나 성숙도를 판단하는 데 유용합니다. 시가총액이 크면 시장이 상대적으로 안정적일 가능성이 높고, 시장 내에서 비트코인의 영향력이 크다는 의미 \n",
    "btc_data = btc_data[['open', 'high', 'low', 'close', 'volume', 'marketCap']] \n",
    "\n",
    "# 레이블(target) 생성 (target = 다음 날의 close 값)..shift(1)은 close값들(열)을 한 칸 내려가게 해서 전날 나머지 값들이 다음 날의 close값을 target으로 잡게 설정한다. 그래야 다음날의 close값을 예측하는 것이기 떄문에\n",
    "btc_data['Target'] = btc_data['close'].shift(1)\n",
    "\n",
    "# 결측값 제거가 무조건 필요하다. 왜냐하면, shift(1)로 target값 열, 즉 close값 행들이 한칸 아래로 내려 갔으므로 첫번째 close값은 결측값이 된다. 결측값이 모델 학습을 방해하므로 없애줍니다. 그리고 텐서플로우 같은 딥러닝 라이브러리는 입력 데이터에 결측값이 있으면 에러가 생김. \n",
    "btc_data = btc_data.dropna()\n",
    "\n",
    "\n",
    "# 특성과 레이블 분리......특성(X): open, high, low, volume, marketCap 특성의 값들을 분리시켜기 위해 values로 설정.....레이블(y): Target 컬럼을 종속 변수로 설정하여 예측하려는 값(다음 날의 close값)을 지정합니다.\n",
    "X = btc_data[['open', 'high', 'low', 'volume', 'marketCap']].values              #.values는 Pandas 데이터프레임이나 시리즈 객체에서 데이터를 NumPy 배열로 변환하는 데 사용된다. 텐서플로우나 다른 머신러닝 라이브러리에서 데이터를 입력받을 때 일반적으로 NumPy 배열 형식을 요구한다. 그래서 .values로 numpy 배열로 값들을 바꿔줍니다 \n",
    "y = btc_data['Target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b5c67c-1dfa-4791-9840-d22730c0f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 분할 및 정규화...이 코드는 데이터 분할 및 정규화 과정에 관한 것으로, 머신러닝 모델을 학습시키기 위한 준비 작업. 각 단계는 데이터를 훈련, 검증, 테스트 세트로 나누고, 데이터를 정규화하여 모델의 성능을 최적화한다.\n",
    "from sklearn.model_selection import train_test_split   #데이터 분할...train_test_split 이라는 함수는 데이터를 랜덤하게 trainset과 testset으로 나누어준다.\n",
    "from sklearn.preprocessing import MinMaxScaler         #데이터 정규화를 도와주는 클래스\n",
    "\n",
    "# 데이터 분할 ...   \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state = 0)   # X를 입력값으로 y를 타겟 값으로 나눔. 전체 데이터의 70%를 학습데이터(X_train, y_train) 와 30%는 테스트/검증 데이터(X_temp, y_temp)로 나눈다.\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state = 0) # trainset중 70% 와 testset30%중에서 반은 검증set, 나머지 반은 testset. trainset은 모델의 가중치를 업데이트하는 데 사용되며, 검증 set은 가중치 업데이트 과정에서 모델 성능을 체크하는 데 사용됩니다. 검증 set은 trainset에 포함되지 않기 때문에 모델의 실제 성능을 평가하는 데 중요한 역할을 한다. 모델의 성능을 최적화하기 위해 하이퍼파라미터(ex. 학습률, 네트워크 층 수, 배치 크기 등)를 조정할 때, 검증 세트를 이용하여 다양한 하이퍼파라미터를 실험하고 그 결과를 평가. 이렇게 해서 검증set 에서 좋은 성과를 보이는 하이퍼파라미터를 선택. 또 과적합 방지 \n",
    "\n",
    "                                          # 정규화.. 데이터의 값을 일정한 범위로 변환하여 모델이 학습할 때 특정 특성에 지나치게 의존하지 않도록 만드는 과정. 특성들은 값의 범위가 다를 수 있는데, 이를 동일한 범위로 조정하는 것이 중요\n",
    "scaler = MinMaxScaler()                   # MinMaxScaler(). 이 클래스는 각 특성의 값을 0과 1사이로 변환\n",
    "X_train = scaler.fit_transform(X_train)   # fit_transform 메서드는 훈련 데이터에 대한 정규화를 학습하고 적용하는데 사용됩니다\n",
    "X_val = scaler.transform(X_val)           # transform 메서드는 이미 학습된 정규화를 검증 세트와 테스트 세트에 적용합니다.\n",
    "X_test = scaler.transform(X_test)         \n",
    "\n",
    "#검증 세트와 테스트 세트는 모델을 학습할 때 사용되지 않기 때문에, 훈련 세트에서 계산된 스케일링 정보를 그대로 적용해야 합니다.\n",
    "# 타겟도 정규화\n",
    "y_scaler = MinMaxScaler()  #똑같이 0과1 사이의 값으로 변환해주는 클래스 호출하고\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1, 1))     # reshape(-1, 1)을 한 이유는 y_train, y_val, y_test는 기본적으로 1D(스칼라) 배열입니다. MinMaxScaler는 2D(벡터) 배열을 요구하므로, 1열 벡터 형태로 변환하는 작업을 합니다.\n",
    "y_val = y_scaler.transform(y_val.reshape(-1, 1))            # y_scaler.fit_transform: 훈련 데이터에 대해서는 fit_transform을 사용해 정규화를 수행하고, 검증 데이터와 테스트 데이터에 대해서는 이미 학습된 scaler를 사용해 transform만 수행합니다\n",
    "y_test = y_scaler.transform(y_test.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e478d904-9082-418c-aced-0bd14948c6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 22:42:24.870812: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#모델 생성하기.. dense layer 알고리즘\n",
    "model = tf.keras.Sequential([                     #Sequential 모델로 순차적인 층을 쌓는다.\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),     #첫번쨰 은닉층으로 128개의 뉴런을 그리고 활성화함수로 ReLU를 사용한다. \n",
    "    tf.keras.layers.Dropout(0.3),                                                       # Dropout : 30%의 뉴런(노드)를 무작위로 dropout 시킨다. 과적합 방지를 위해 \n",
    "    tf.keras.layers.Dense(64, activation='relu'),                                       #두번쨰 은닉층(hidden layer)으로 64개의 뉴런, 활성화함수 ReLU \n",
    "    tf.keras.layers.Dropout(0.3),                                                        #Dropout\n",
    "    tf.keras.layers.Dense(32, activation='relu'),                                        #세번째 은닉층 : 노드 32개, 활성화함수 ReLU\n",
    "    tf.keras.layers.Dense(1)  # 연속형 출력                         #출력층,,회귀문제(연속적인 값을 예측)이기 떄문에 예측값이 그대로 나와야하기 떄문에 활성화함수는 없다.별도의 비선형 변환을 추가하지 않는다.선형 출력.출력값을 그대로 나오게한다      활성화함수로 sigmoid를 사용하면 출력값이 [0,1]로 제한된다.\n",
    "])\n",
    "# 층을 더 쌓았었는데 오히려 test loss가 안좋게 나와서 줄였습니다\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])          #모델 컴파일 할때는 최적화함수(optimizer)를 Adam(학습률을 조정해주는, 경사하강법과 같은 부류의 알고리즘)으로 설정하고 손실함수(loss function)은 MSE(Mean squared Error, 평균 제곱 오차: 회귀 모델의 성능을 평가하는데 사용하는 대표적인 손실 함수. 모델 예측한 값과 실제 값 차이를 제곱하여 평균을 구한 값)를 사용한다. 예측 오차의 절대값 평균을 계산한다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3399b7df-da96-4ac0-ae3f-60c5e7f678ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 66ms/step - loss: 0.2148 - mae: 0.4138 - val_loss: 0.1425 - val_mae: 0.2972\n",
      "Epoch 2/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0964 - mae: 0.2633 - val_loss: 0.0348 - val_mae: 0.1530\n",
      "Epoch 3/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0302 - mae: 0.1399 - val_loss: 0.0178 - val_mae: 0.1151\n",
      "Epoch 4/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0238 - mae: 0.1297 - val_loss: 0.0100 - val_mae: 0.0781\n",
      "Epoch 5/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0143 - mae: 0.0953 - val_loss: 0.0111 - val_mae: 0.0876\n",
      "Epoch 6/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0141 - mae: 0.0943 - val_loss: 0.0092 - val_mae: 0.0798\n",
      "Epoch 7/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0108 - mae: 0.0768 - val_loss: 0.0063 - val_mae: 0.0660\n",
      "Epoch 8/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0100 - mae: 0.0759 - val_loss: 0.0063 - val_mae: 0.0663\n",
      "Epoch 9/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0090 - mae: 0.0721 - val_loss: 0.0069 - val_mae: 0.0678\n",
      "Epoch 10/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0630 - val_loss: 0.0051 - val_mae: 0.0577\n",
      "Epoch 11/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0060 - mae: 0.0590 - val_loss: 0.0040 - val_mae: 0.0505\n",
      "Epoch 12/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0075 - mae: 0.0633 - val_loss: 0.0045 - val_mae: 0.0539\n",
      "Epoch 13/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mae: 0.0567 - val_loss: 0.0021 - val_mae: 0.0359\n",
      "Epoch 14/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0065 - mae: 0.0636 - val_loss: 0.0074 - val_mae: 0.0695\n",
      "Epoch 15/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0059 - mae: 0.0570 - val_loss: 0.0033 - val_mae: 0.0453\n",
      "Epoch 16/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0063 - mae: 0.0622 - val_loss: 0.0039 - val_mae: 0.0491\n",
      "Epoch 17/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0045 - mae: 0.0499 - val_loss: 0.0073 - val_mae: 0.0691\n",
      "Epoch 18/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0054 - mae: 0.0539 - val_loss: 0.0031 - val_mae: 0.0438\n",
      "Epoch 19/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0048 - mae: 0.0519 - val_loss: 0.0053 - val_mae: 0.0582\n",
      "Epoch 20/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0051 - mae: 0.0557 - val_loss: 0.0030 - val_mae: 0.0426\n",
      "Epoch 21/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0072 - mae: 0.0596 - val_loss: 0.0038 - val_mae: 0.0485\n",
      "Epoch 22/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - mae: 0.0535 - val_loss: 0.0045 - val_mae: 0.0527\n",
      "Epoch 23/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0062 - mae: 0.0560 - val_loss: 0.0043 - val_mae: 0.0510\n",
      "Epoch 24/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0490 - val_loss: 0.0040 - val_mae: 0.0488\n",
      "Epoch 25/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0050 - mae: 0.0519 - val_loss: 0.0046 - val_mae: 0.0520\n",
      "Epoch 26/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mae: 0.0444 - val_loss: 0.0063 - val_mae: 0.0616\n",
      "Epoch 27/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0043 - mae: 0.0501 - val_loss: 0.0034 - val_mae: 0.0439\n",
      "Epoch 28/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0043 - mae: 0.0488 - val_loss: 0.0079 - val_mae: 0.0696\n",
      "Epoch 29/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mae: 0.0490 - val_loss: 0.0035 - val_mae: 0.0448\n",
      "Epoch 30/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - mae: 0.0522 - val_loss: 0.0057 - val_mae: 0.0580\n",
      "Epoch 31/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0040 - mae: 0.0483 - val_loss: 0.0053 - val_mae: 0.0560\n",
      "Epoch 32/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0045 - mae: 0.0508 - val_loss: 0.0046 - val_mae: 0.0521\n",
      "Epoch 33/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - mae: 0.0540 - val_loss: 0.0064 - val_mae: 0.0621\n",
      "Epoch 34/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0039 - mae: 0.0471 - val_loss: 0.0045 - val_mae: 0.0517\n",
      "Epoch 35/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0040 - mae: 0.0458 - val_loss: 0.0058 - val_mae: 0.0590\n",
      "Epoch 36/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mae: 0.0419 - val_loss: 0.0064 - val_mae: 0.0622\n",
      "Epoch 37/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0040 - mae: 0.0441 - val_loss: 0.0053 - val_mae: 0.0562\n",
      "Epoch 38/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0040 - mae: 0.0440 - val_loss: 0.0052 - val_mae: 0.0564\n",
      "Epoch 39/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0031 - mae: 0.0411 - val_loss: 0.0065 - val_mae: 0.0640\n",
      "Epoch 40/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0030 - mae: 0.0427 - val_loss: 0.0043 - val_mae: 0.0513\n",
      "Epoch 41/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0035 - mae: 0.0439 - val_loss: 0.0045 - val_mae: 0.0519\n",
      "Epoch 42/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0031 - mae: 0.0421 - val_loss: 0.0050 - val_mae: 0.0546\n",
      "Epoch 43/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0032 - mae: 0.0413 - val_loss: 0.0079 - val_mae: 0.0697\n",
      "Epoch 44/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0415 - val_loss: 0.0055 - val_mae: 0.0575\n",
      "Epoch 45/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0038 - mae: 0.0453 - val_loss: 0.0063 - val_mae: 0.0611\n",
      "Epoch 46/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0032 - mae: 0.0441 - val_loss: 0.0055 - val_mae: 0.0579\n",
      "Epoch 47/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mae: 0.0384 - val_loss: 0.0058 - val_mae: 0.0601\n",
      "Epoch 48/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0027 - mae: 0.0409 - val_loss: 0.0067 - val_mae: 0.0653\n",
      "Epoch 49/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0032 - mae: 0.0402 - val_loss: 0.0044 - val_mae: 0.0524\n",
      "Epoch 50/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0031 - mae: 0.0430 - val_loss: 0.0051 - val_mae: 0.0561\n",
      "Epoch 51/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mae: 0.0419 - val_loss: 0.0067 - val_mae: 0.0641\n",
      "Epoch 52/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0024 - mae: 0.0346 - val_loss: 0.0040 - val_mae: 0.0494\n",
      "Epoch 53/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0045 - mae: 0.0468 - val_loss: 0.0059 - val_mae: 0.0604\n",
      "Epoch 54/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mae: 0.0375 - val_loss: 0.0049 - val_mae: 0.0553\n",
      "Epoch 55/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mae: 0.0411 - val_loss: 0.0065 - val_mae: 0.0640\n",
      "Epoch 56/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0030 - mae: 0.0416 - val_loss: 0.0034 - val_mae: 0.0461\n",
      "Epoch 57/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mae: 0.0431 - val_loss: 0.0047 - val_mae: 0.0543\n",
      "Epoch 58/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0027 - mae: 0.0393 - val_loss: 0.0045 - val_mae: 0.0530\n",
      "Epoch 59/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0026 - mae: 0.0384 - val_loss: 0.0047 - val_mae: 0.0543\n",
      "Epoch 60/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0034 - mae: 0.0439 - val_loss: 0.0044 - val_mae: 0.0525\n",
      "Epoch 61/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0024 - mae: 0.0363 - val_loss: 0.0052 - val_mae: 0.0572\n",
      "Epoch 62/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0028 - mae: 0.0391 - val_loss: 0.0028 - val_mae: 0.0424\n",
      "Epoch 63/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0035 - mae: 0.0427 - val_loss: 0.0052 - val_mae: 0.0581\n",
      "Epoch 64/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0359 - val_loss: 0.0049 - val_mae: 0.0565\n",
      "Epoch 65/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0023 - mae: 0.0362 - val_loss: 0.0039 - val_mae: 0.0499\n",
      "Epoch 66/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0367 - val_loss: 0.0049 - val_mae: 0.0557\n",
      "Epoch 67/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0027 - mae: 0.0380 - val_loss: 0.0028 - val_mae: 0.0434\n",
      "Epoch 68/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0028 - mae: 0.0412 - val_loss: 0.0053 - val_mae: 0.0592\n",
      "Epoch 69/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mae: 0.0423 - val_loss: 0.0052 - val_mae: 0.0586\n",
      "Epoch 70/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0040 - mae: 0.0422 - val_loss: 0.0038 - val_mae: 0.0502\n",
      "Epoch 71/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0368 - val_loss: 0.0058 - val_mae: 0.0614\n",
      "Epoch 72/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0023 - mae: 0.0336 - val_loss: 0.0058 - val_mae: 0.0604\n",
      "Epoch 73/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0029 - mae: 0.0414 - val_loss: 0.0048 - val_mae: 0.0554\n",
      "Epoch 74/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0377 - val_loss: 0.0069 - val_mae: 0.0657\n",
      "Epoch 75/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0032 - mae: 0.0386 - val_loss: 0.0029 - val_mae: 0.0432\n",
      "Epoch 76/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023 - mae: 0.0355 - val_loss: 0.0056 - val_mae: 0.0605\n",
      "Epoch 77/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mae: 0.0371 - val_loss: 0.0033 - val_mae: 0.0475\n",
      "Epoch 78/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0022 - mae: 0.0346 - val_loss: 0.0068 - val_mae: 0.0661\n",
      "Epoch 79/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0023 - mae: 0.0361 - val_loss: 0.0046 - val_mae: 0.0551\n",
      "Epoch 80/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - mae: 0.0387 - val_loss: 0.0060 - val_mae: 0.0619\n",
      "Epoch 81/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - mae: 0.0347 - val_loss: 0.0035 - val_mae: 0.0475\n",
      "Epoch 82/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0373 - val_loss: 0.0037 - val_mae: 0.0489\n",
      "Epoch 83/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0025 - mae: 0.0370 - val_loss: 0.0039 - val_mae: 0.0513\n",
      "Epoch 84/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0028 - mae: 0.0403 - val_loss: 0.0041 - val_mae: 0.0532\n",
      "Epoch 85/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - mae: 0.0348 - val_loss: 0.0042 - val_mae: 0.0528\n",
      "Epoch 86/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0330 - val_loss: 0.0044 - val_mae: 0.0533\n",
      "Epoch 87/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0371 - val_loss: 0.0049 - val_mae: 0.0555\n",
      "Epoch 88/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0023 - mae: 0.0341 - val_loss: 0.0038 - val_mae: 0.0494\n",
      "Epoch 89/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0023 - mae: 0.0359 - val_loss: 0.0043 - val_mae: 0.0530\n",
      "Epoch 90/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - mae: 0.0339 - val_loss: 0.0031 - val_mae: 0.0463\n",
      "Epoch 91/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0031 - mae: 0.0410 - val_loss: 0.0046 - val_mae: 0.0555\n",
      "Epoch 92/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0027 - mae: 0.0353 - val_loss: 0.0035 - val_mae: 0.0487\n",
      "Epoch 93/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - mae: 0.0351 - val_loss: 0.0057 - val_mae: 0.0614\n",
      "Epoch 94/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025 - mae: 0.0363 - val_loss: 0.0048 - val_mae: 0.0557\n",
      "Epoch 95/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0332 - val_loss: 0.0040 - val_mae: 0.0514\n",
      "Epoch 96/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0026 - mae: 0.0349 - val_loss: 0.0024 - val_mae: 0.0411\n",
      "Epoch 97/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0027 - mae: 0.0395 - val_loss: 0.0066 - val_mae: 0.0659\n",
      "Epoch 98/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0023 - mae: 0.0351 - val_loss: 0.0027 - val_mae: 0.0445\n",
      "Epoch 99/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0024 - mae: 0.0358 - val_loss: 0.0043 - val_mae: 0.0552\n",
      "Epoch 100/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0332 - val_loss: 0.0030 - val_mae: 0.0462\n",
      "Epoch 101/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mae: 0.0370 - val_loss: 0.0040 - val_mae: 0.0522\n",
      "Epoch 102/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019 - mae: 0.0322 - val_loss: 0.0026 - val_mae: 0.0435\n",
      "Epoch 103/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mae: 0.0347 - val_loss: 0.0051 - val_mae: 0.0588\n",
      "Epoch 104/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0020 - mae: 0.0345 - val_loss: 0.0042 - val_mae: 0.0531\n",
      "Epoch 105/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0024 - mae: 0.0364 - val_loss: 0.0042 - val_mae: 0.0529\n",
      "Epoch 106/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.0327 - val_loss: 0.0044 - val_mae: 0.0539\n",
      "Epoch 107/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 0.0040 - val_mae: 0.0515\n",
      "Epoch 108/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0024 - mae: 0.0372 - val_loss: 0.0052 - val_mae: 0.0582\n",
      "Epoch 109/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020 - mae: 0.0323 - val_loss: 0.0046 - val_mae: 0.0553\n",
      "Epoch 110/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0326 - val_loss: 0.0044 - val_mae: 0.0543\n",
      "Epoch 111/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0016 - mae: 0.0305 - val_loss: 0.0036 - val_mae: 0.0499\n",
      "Epoch 112/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0026 - mae: 0.0352 - val_loss: 0.0043 - val_mae: 0.0548\n",
      "Epoch 113/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.0050 - val_mae: 0.0584\n",
      "Epoch 114/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0021 - mae: 0.0345 - val_loss: 0.0031 - val_mae: 0.0475\n",
      "Epoch 115/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0024 - mae: 0.0349 - val_loss: 0.0049 - val_mae: 0.0581\n",
      "Epoch 116/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0025 - mae: 0.0346 - val_loss: 0.0042 - val_mae: 0.0539\n",
      "Epoch 117/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - mae: 0.0308 - val_loss: 0.0042 - val_mae: 0.0545\n",
      "Epoch 118/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0022 - mae: 0.0344 - val_loss: 0.0028 - val_mae: 0.0450\n",
      "Epoch 119/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - mae: 0.0342 - val_loss: 0.0039 - val_mae: 0.0530\n",
      "Epoch 120/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - mae: 0.0321 - val_loss: 0.0043 - val_mae: 0.0546\n",
      "Epoch 121/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0331 - val_loss: 0.0033 - val_mae: 0.0472\n",
      "Epoch 122/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0335 - val_loss: 0.0042 - val_mae: 0.0524\n",
      "Epoch 123/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0022 - mae: 0.0359 - val_loss: 0.0038 - val_mae: 0.0500\n",
      "Epoch 124/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0295 - val_loss: 0.0042 - val_mae: 0.0522\n",
      "Epoch 125/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - mae: 0.0354 - val_loss: 0.0030 - val_mae: 0.0453\n",
      "Epoch 126/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - mae: 0.0360 - val_loss: 0.0040 - val_mae: 0.0526\n",
      "Epoch 127/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0027 - mae: 0.0360 - val_loss: 0.0031 - val_mae: 0.0473\n",
      "Epoch 128/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0033 - val_mae: 0.0489\n",
      "Epoch 129/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0359 - val_loss: 0.0043 - val_mae: 0.0552\n",
      "Epoch 130/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0024 - mae: 0.0353 - val_loss: 0.0036 - val_mae: 0.0502\n",
      "Epoch 131/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 0.0038 - val_mae: 0.0511\n",
      "Epoch 132/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mae: 0.0350 - val_loss: 0.0034 - val_mae: 0.0483\n",
      "Epoch 133/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0322 - val_loss: 0.0032 - val_mae: 0.0475\n",
      "Epoch 134/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - mae: 0.0338 - val_loss: 0.0028 - val_mae: 0.0455\n",
      "Epoch 135/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - mae: 0.0321 - val_loss: 0.0035 - val_mae: 0.0509\n",
      "Epoch 136/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0293 - val_loss: 0.0028 - val_mae: 0.0453\n",
      "Epoch 137/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0025 - mae: 0.0327 - val_loss: 0.0035 - val_mae: 0.0498\n",
      "Epoch 138/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - mae: 0.0340 - val_loss: 0.0027 - val_mae: 0.0441\n",
      "Epoch 139/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0019 - mae: 0.0325 - val_loss: 0.0033 - val_mae: 0.0486\n",
      "Epoch 140/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0331 - val_loss: 0.0030 - val_mae: 0.0474\n",
      "Epoch 141/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019 - mae: 0.0308 - val_loss: 0.0048 - val_mae: 0.0576\n",
      "Epoch 142/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018 - mae: 0.0323 - val_loss: 0.0044 - val_mae: 0.0539\n",
      "Epoch 143/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0024 - mae: 0.0359 - val_loss: 0.0033 - val_mae: 0.0462\n",
      "Epoch 144/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 0.0042 - val_mae: 0.0524\n",
      "Epoch 145/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0022 - mae: 0.0335 - val_loss: 0.0031 - val_mae: 0.0475\n",
      "Epoch 146/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0305 - val_loss: 0.0043 - val_mae: 0.0553\n",
      "Epoch 147/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mae: 0.0360 - val_loss: 0.0036 - val_mae: 0.0503\n",
      "Epoch 148/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0306 - val_loss: 0.0036 - val_mae: 0.0497\n",
      "Epoch 149/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020 - mae: 0.0329 - val_loss: 0.0046 - val_mae: 0.0554\n",
      "Epoch 150/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0293 - val_loss: 0.0028 - val_mae: 0.0445\n",
      "Epoch 151/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0018 - mae: 0.0310 - val_loss: 0.0036 - val_mae: 0.0495\n",
      "Epoch 152/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - mae: 0.0321 - val_loss: 0.0030 - val_mae: 0.0463\n",
      "Epoch 153/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0304 - val_loss: 0.0043 - val_mae: 0.0549\n",
      "Epoch 154/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0039 - val_mae: 0.0523\n",
      "Epoch 155/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - mae: 0.0322 - val_loss: 0.0027 - val_mae: 0.0434\n",
      "Epoch 156/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0017 - mae: 0.0322 - val_loss: 0.0035 - val_mae: 0.0491\n",
      "Epoch 157/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0019 - mae: 0.0325 - val_loss: 0.0035 - val_mae: 0.0499\n",
      "Epoch 158/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0019 - mae: 0.0319 - val_loss: 0.0034 - val_mae: 0.0495\n",
      "Epoch 159/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0300 - val_loss: 0.0028 - val_mae: 0.0455\n",
      "Epoch 160/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0317 - val_loss: 0.0042 - val_mae: 0.0538\n",
      "Epoch 161/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0288 - val_loss: 0.0029 - val_mae: 0.0449\n",
      "Epoch 162/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0294 - val_loss: 0.0033 - val_mae: 0.0473\n",
      "Epoch 163/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0314 - val_loss: 0.0028 - val_mae: 0.0454\n",
      "Epoch 164/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019 - mae: 0.0331 - val_loss: 0.0033 - val_mae: 0.0492\n",
      "Epoch 165/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0306 - val_loss: 0.0026 - val_mae: 0.0444\n",
      "Epoch 166/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0315 - val_loss: 0.0037 - val_mae: 0.0502\n",
      "Epoch 167/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0309 - val_loss: 0.0029 - val_mae: 0.0446\n",
      "Epoch 168/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0014 - mae: 0.0284 - val_loss: 0.0033 - val_mae: 0.0486\n",
      "Epoch 169/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0016 - mae: 0.0301 - val_loss: 0.0027 - val_mae: 0.0448\n",
      "Epoch 170/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0016 - mae: 0.0295 - val_loss: 0.0036 - val_mae: 0.0501\n",
      "Epoch 171/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - mae: 0.0331 - val_loss: 0.0029 - val_mae: 0.0448\n",
      "Epoch 172/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0031 - val_mae: 0.0462\n",
      "Epoch 173/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0014 - mae: 0.0300 - val_loss: 0.0031 - val_mae: 0.0466\n",
      "Epoch 174/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 0.0033 - val_mae: 0.0485\n",
      "Epoch 175/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - mae: 0.0293 - val_loss: 0.0022 - val_mae: 0.0402\n",
      "Epoch 176/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0032 - val_mae: 0.0474\n",
      "Epoch 177/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 0.0030 - val_mae: 0.0458\n",
      "Epoch 178/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0315 - val_loss: 0.0030 - val_mae: 0.0459\n",
      "Epoch 179/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - mae: 0.0320 - val_loss: 0.0027 - val_mae: 0.0445\n",
      "Epoch 180/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 0.0027 - val_mae: 0.0454\n",
      "Epoch 181/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - mae: 0.0325 - val_loss: 0.0034 - val_mae: 0.0499\n",
      "Epoch 182/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0293 - val_loss: 0.0021 - val_mae: 0.0382\n",
      "Epoch 183/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0018 - mae: 0.0327 - val_loss: 0.0025 - val_mae: 0.0414\n",
      "Epoch 184/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0267 - val_loss: 0.0020 - val_mae: 0.0377\n",
      "Epoch 185/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - mae: 0.0311 - val_loss: 0.0046 - val_mae: 0.0556\n",
      "Epoch 186/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0022 - mae: 0.0334 - val_loss: 0.0023 - val_mae: 0.0404\n",
      "Epoch 187/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0274 - val_loss: 0.0037 - val_mae: 0.0487\n",
      "Epoch 188/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0016 - mae: 0.0304 - val_loss: 0.0026 - val_mae: 0.0420\n",
      "Epoch 189/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0341 - val_loss: 0.0029 - val_mae: 0.0445\n",
      "Epoch 190/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0020 - mae: 0.0315 - val_loss: 0.0029 - val_mae: 0.0442\n",
      "Epoch 191/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0298 - val_loss: 0.0032 - val_mae: 0.0458\n",
      "Epoch 192/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0016 - mae: 0.0307 - val_loss: 0.0025 - val_mae: 0.0413\n",
      "Epoch 193/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0280 - val_loss: 0.0033 - val_mae: 0.0470\n",
      "Epoch 194/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.0037 - val_mae: 0.0495\n",
      "Epoch 195/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020 - mae: 0.0314 - val_loss: 0.0019 - val_mae: 0.0362\n",
      "Epoch 196/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0321 - val_loss: 0.0037 - val_mae: 0.0505\n",
      "Epoch 197/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0021 - mae: 0.0341 - val_loss: 0.0026 - val_mae: 0.0432\n",
      "Epoch 198/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0297 - val_loss: 0.0053 - val_mae: 0.0562\n",
      "Epoch 199/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mae: 0.0326 - val_loss: 0.0022 - val_mae: 0.0382\n",
      "Epoch 200/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0018 - mae: 0.0320 - val_loss: 0.0046 - val_mae: 0.0536\n",
      "Epoch 201/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0311 - val_loss: 0.0020 - val_mae: 0.0376\n",
      "Epoch 202/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0014 - mae: 0.0295 - val_loss: 0.0030 - val_mae: 0.0457\n",
      "Epoch 203/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0018 - mae: 0.0317 - val_loss: 0.0022 - val_mae: 0.0391\n",
      "Epoch 204/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mae: 0.0328 - val_loss: 0.0031 - val_mae: 0.0459\n",
      "Epoch 205/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0019 - mae: 0.0309 - val_loss: 0.0039 - val_mae: 0.0500\n",
      "Epoch 206/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0313 - val_loss: 0.0031 - val_mae: 0.0441\n",
      "Epoch 207/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0321 - val_loss: 0.0026 - val_mae: 0.0415\n",
      "Epoch 208/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0019 - mae: 0.0307 - val_loss: 0.0019 - val_mae: 0.0363\n",
      "Epoch 209/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0016 - mae: 0.0291 - val_loss: 0.0042 - val_mae: 0.0521\n",
      "Epoch 210/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0016 - val_mae: 0.0315\n",
      "Epoch 211/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0331 - val_loss: 0.0040 - val_mae: 0.0499\n",
      "Epoch 212/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0309 - val_loss: 0.0023 - val_mae: 0.0394\n",
      "Epoch 213/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0013 - mae: 0.0289 - val_loss: 0.0030 - val_mae: 0.0446\n",
      "Epoch 214/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0290 - val_loss: 0.0025 - val_mae: 0.0409\n",
      "Epoch 215/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 0.0026 - val_mae: 0.0410\n",
      "Epoch 216/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - mae: 0.0318 - val_loss: 0.0036 - val_mae: 0.0469\n",
      "Epoch 217/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0015 - mae: 0.0289 - val_loss: 0.0026 - val_mae: 0.0405\n",
      "Epoch 218/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0015 - mae: 0.0300 - val_loss: 0.0030 - val_mae: 0.0441\n",
      "Epoch 219/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019 - mae: 0.0298 - val_loss: 0.0028 - val_mae: 0.0429\n",
      "Epoch 220/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - mae: 0.0325 - val_loss: 0.0030 - val_mae: 0.0439\n",
      "Epoch 221/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0029 - val_mae: 0.0432\n",
      "Epoch 222/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 0.0034 - val_mae: 0.0464\n",
      "Epoch 223/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0020 - mae: 0.0316 - val_loss: 0.0027 - val_mae: 0.0413\n",
      "Epoch 224/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0012 - mae: 0.0276 - val_loss: 0.0035 - val_mae: 0.0461\n",
      "Epoch 225/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - mae: 0.0335 - val_loss: 0.0024 - val_mae: 0.0391\n",
      "Epoch 226/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - mae: 0.0314 - val_loss: 0.0042 - val_mae: 0.0487\n",
      "Epoch 227/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0030 - val_mae: 0.0415\n",
      "Epoch 228/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0013 - mae: 0.0268 - val_loss: 0.0031 - val_mae: 0.0430\n",
      "Epoch 229/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0015 - mae: 0.0277 - val_loss: 0.0023 - val_mae: 0.0393\n",
      "Epoch 230/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - mae: 0.0277 - val_loss: 0.0034 - val_mae: 0.0455\n",
      "Epoch 231/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0297 - val_loss: 0.0034 - val_mae: 0.0445\n",
      "Epoch 232/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0285 - val_loss: 0.0033 - val_mae: 0.0438\n",
      "Epoch 233/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0012 - mae: 0.0274 - val_loss: 0.0031 - val_mae: 0.0426\n",
      "Epoch 234/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0296 - val_loss: 0.0029 - val_mae: 0.0418\n",
      "Epoch 235/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0287 - val_loss: 0.0027 - val_mae: 0.0402\n",
      "Epoch 236/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0299 - val_loss: 0.0025 - val_mae: 0.0386\n",
      "Epoch 237/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0017 - mae: 0.0325 - val_loss: 0.0033 - val_mae: 0.0454\n",
      "Epoch 238/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0021 - mae: 0.0331 - val_loss: 0.0022 - val_mae: 0.0373\n",
      "Epoch 239/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0304 - val_loss: 0.0036 - val_mae: 0.0467\n",
      "Epoch 240/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0294 - val_loss: 0.0034 - val_mae: 0.0448\n",
      "Epoch 241/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 0.0033 - val_mae: 0.0435\n",
      "Epoch 242/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0313 - val_loss: 0.0034 - val_mae: 0.0438\n",
      "Epoch 243/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0015 - mae: 0.0291 - val_loss: 0.0037 - val_mae: 0.0464\n",
      "Epoch 244/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0014 - mae: 0.0287 - val_loss: 0.0027 - val_mae: 0.0403\n",
      "Epoch 245/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0304 - val_loss: 0.0036 - val_mae: 0.0467\n",
      "Epoch 246/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0294 - val_loss: 0.0035 - val_mae: 0.0456\n",
      "Epoch 247/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 0.0021 - val_mae: 0.0360\n",
      "Epoch 248/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0334 - val_loss: 0.0055 - val_mae: 0.0550\n",
      "Epoch 249/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0019 - mae: 0.0320 - val_loss: 0.0020 - val_mae: 0.0331\n",
      "Epoch 250/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0016 - mae: 0.0315 - val_loss: 0.0029 - val_mae: 0.0427\n",
      "Epoch 251/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - mae: 0.0306 - val_loss: 0.0019 - val_mae: 0.0355\n",
      "Epoch 252/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0261 - val_loss: 0.0028 - val_mae: 0.0422\n",
      "Epoch 253/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 0.0025 - val_mae: 0.0386\n",
      "Epoch 254/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0284 - val_loss: 0.0034 - val_mae: 0.0455\n",
      "Epoch 255/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018 - mae: 0.0287 - val_loss: 0.0022 - val_mae: 0.0382\n",
      "Epoch 256/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.0025 - val_mae: 0.0408\n",
      "Epoch 257/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0298 - val_loss: 0.0033 - val_mae: 0.0455\n",
      "Epoch 258/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.0019 - mae: 0.0296 - val_loss: 0.0030 - val_mae: 0.0424\n",
      "Epoch 259/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0274 - val_loss: 0.0027 - val_mae: 0.0406\n",
      "Epoch 260/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0281 - val_loss: 0.0023 - val_mae: 0.0387\n",
      "Epoch 261/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0015 - mae: 0.0279 - val_loss: 0.0039 - val_mae: 0.0498\n",
      "Epoch 262/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 0.0025 - val_mae: 0.0390\n",
      "Epoch 263/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - mae: 0.0320 - val_loss: 0.0034 - val_mae: 0.0442\n",
      "Epoch 264/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011 - mae: 0.0253 - val_loss: 0.0028 - val_mae: 0.0412\n",
      "Epoch 265/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0013 - mae: 0.0271 - val_loss: 0.0027 - val_mae: 0.0414\n",
      "Epoch 266/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0012 - mae: 0.0270 - val_loss: 0.0022 - val_mae: 0.0378\n",
      "Epoch 267/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0018 - mae: 0.0301 - val_loss: 0.0030 - val_mae: 0.0434\n",
      "Epoch 268/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0285 - val_loss: 0.0023 - val_mae: 0.0379\n",
      "Epoch 269/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0032 - val_mae: 0.0438\n",
      "Epoch 270/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0260 - val_loss: 0.0019 - val_mae: 0.0340\n",
      "Epoch 271/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0017 - mae: 0.0309 - val_loss: 0.0029 - val_mae: 0.0417\n",
      "Epoch 272/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0016 - mae: 0.0288 - val_loss: 0.0021 - val_mae: 0.0358\n",
      "Epoch 273/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0294 - val_loss: 0.0027 - val_mae: 0.0402\n",
      "Epoch 274/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0022 - mae: 0.0336 - val_loss: 0.0025 - val_mae: 0.0394\n",
      "Epoch 275/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.0014 - mae: 0.0283 - val_loss: 0.0024 - val_mae: 0.0383\n",
      "Epoch 276/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0273 - val_loss: 0.0030 - val_mae: 0.0417\n",
      "Epoch 277/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0278 - val_loss: 0.0020 - val_mae: 0.0348\n",
      "Epoch 278/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0014 - mae: 0.0290 - val_loss: 0.0022 - val_mae: 0.0363\n",
      "Epoch 279/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0011 - mae: 0.0263 - val_loss: 0.0028 - val_mae: 0.0404\n",
      "Epoch 280/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - mae: 0.0304 - val_loss: 0.0020 - val_mae: 0.0346\n",
      "Epoch 281/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0273 - val_loss: 0.0021 - val_mae: 0.0367\n",
      "Epoch 282/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0011 - mae: 0.0249 - val_loss: 0.0017 - val_mae: 0.0336\n",
      "Epoch 283/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - mae: 0.0308 - val_loss: 0.0028 - val_mae: 0.0422\n",
      "Epoch 284/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 0.0022 - val_mae: 0.0365\n",
      "Epoch 285/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0016 - mae: 0.0304 - val_loss: 0.0027 - val_mae: 0.0397\n",
      "Epoch 286/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0016 - mae: 0.0302 - val_loss: 0.0028 - val_mae: 0.0416\n",
      "Epoch 287/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0016 - mae: 0.0294 - val_loss: 0.0028 - val_mae: 0.0414\n",
      "Epoch 288/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0020 - mae: 0.0310 - val_loss: 0.0020 - val_mae: 0.0350\n",
      "Epoch 289/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0017 - mae: 0.0306 - val_loss: 0.0035 - val_mae: 0.0450\n",
      "Epoch 290/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0018 - mae: 0.0307 - val_loss: 0.0025 - val_mae: 0.0360\n",
      "Epoch 291/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0014 - mae: 0.0304 - val_loss: 0.0031 - val_mae: 0.0411\n",
      "Epoch 292/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0271 - val_loss: 0.0028 - val_mae: 0.0409\n",
      "Epoch 293/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - mae: 0.0266 - val_loss: 0.0021 - val_mae: 0.0358\n",
      "Epoch 294/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0017 - mae: 0.0296 - val_loss: 0.0021 - val_mae: 0.0371\n",
      "Epoch 295/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0013 - mae: 0.0265 - val_loss: 0.0018 - val_mae: 0.0337\n",
      "Epoch 296/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0302 - val_loss: 0.0034 - val_mae: 0.0461\n",
      "Epoch 297/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 0.0021 - val_mae: 0.0368\n",
      "Epoch 298/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0013 - mae: 0.0282 - val_loss: 0.0027 - val_mae: 0.0399\n",
      "Epoch 299/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0012 - mae: 0.0258 - val_loss: 0.0028 - val_mae: 0.0393\n",
      "Epoch 300/300\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0014 - mae: 0.0270 - val_loss: 0.0040 - val_mae: 0.0467\n"
     ]
    }
   ],
   "source": [
    "# 위에서 만든 모델로 훈련하기 model.fit\n",
    "history = model.fit(                        # 딥러닝 모델을 훈련시킨다. model.fit()메서드는 주어진 데이터와 설정을 기반으로 모델을 학습\n",
    "    X_train, y_train,                        # X_train은 입력 데이터(특징),,y_train은 target 데이터(레이블),실제 값들\n",
    "    validation_data=(X_val, y_val),          # (validation_data) = 검증 데이터 로 학습과정에서 모델 성능을 평가한다.\n",
    "    epochs=300,                              # 전체 데이터셋으로 300번을 반복하여 학습하게 하고 , 배치 크기는 32로 설정합니다.\n",
    "    batch_size=32                            # 데이터를 전체 데이터에서 32개씩 나누어서 처리한다.   배치학습 - 메모리 사용량 줄이고 효율적인 학습을 돕는다\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ec7749-81bc-4f55-a264-cc888f3dc164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0013 - mae: 0.0272\n",
      "Test Loss: 0.0013239339459687471, Test MAE: 0.02771766670048237\n"
     ]
    }
   ],
   "source": [
    "# test set으로 모델 평가하기\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)              # 위에서 만든 모델을 test data(X_test, y_test) 로 평가한다. model.evaluate() 는 모델이 테스트 데이터에서의 성능을 평가하는데 사용된다. 그리고 출력값들을 test_loss:모델이 예측한 값과 실제 값 간의 손실값, test_mae: 테스트 데이터에서 계산된 평가 지표포 평균 절대 오차를 말한다. \n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")           #test_loss 는 테스트 데이터에 대해 계산된 손실 값이다. loss function은 위에서 설정한 평균 제곱 오차(mse)이다. test_mae 는 모델이 테스트 데이터에서 얼마나 정확히 예측했는지 측정한다. MAE는 예측값과 실제 값 사이의 차이들의 평균의 절대값을 나타낸다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3962af-f460-4ee3-bc24-2671de23c12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
